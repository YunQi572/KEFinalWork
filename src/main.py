"""
æ¾æçº¿è™«ç—…çŸ¥è¯†å›¾è°±ç³»ç»Ÿ - FastAPIåç«¯
"""
from fastapi import FastAPI, HTTPException, File, UploadFile, Form, Query, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import pymysql
import logging
from contextlib import contextmanager
import os
from pathlib import Path
import time
import uvicorn
import json

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    env_path = Path(__file__).parent / '.env'
    if env_path.exists():
        load_dotenv(env_path)
        logger.info("ç¯å¢ƒå˜é‡åŠ è½½æˆåŠŸ")
except ImportError:
    pass  # python-dotenvæœªå®‰è£…ï¼Œè·³è¿‡

app = FastAPI(title="çŸ¥è¯†å›¾è°±API", version="1.0.0")

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒè¯·ä¿®æ”¹ä¸ºå…·ä½“å‰ç«¯åœ°å€
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# MySQLæ•°æ®åº“é…ç½®
DB_CONFIG = {
    'host': 'localhost',
    'port': 3306,
    'user': 'root',
    'password': '20050702g',
    'database': 'kefinalwork',
    'charset': 'utf8mb4',
    'cursorclass': pymysql.cursors.DictCursor
}

HIGH_LEVEL_NODE_TABLE = "graph_high_level_nodes"

_CORE_HIGH_LEVEL_NODE_RECORDS = [
    {"node_name": "æ¾æçº¿è™«ç—…", "node_type": "core", "description": "æ ¸å¿ƒç—…å®³æ¦‚å¿µ"},
    {"node_name": "æ¾æçº¿è™«", "node_type": "core", "description": "ä¸»è¦ç—…åŸçº¿è™«"},
    {"node_name": "æ¾å¢¨å¤©ç‰›", "node_type": "core", "description": "é‡è¦åª’ä»‹æ˜†è™«"},
    {"node_name": "å¯„ä¸»", "node_type": "core", "description": "å®¿ä¸»æ•´ä½“æ¦‚å¿µ"},
    {"node_name": "åª’ä»‹æ˜†è™«", "node_type": "core", "description": "åª’ä»‹æ€»ä½“ç±»åˆ«"},
]

_GENERIC_HIGH_LEVEL_NODE_NAMES = [
    'çœä»½', 'åŸå¸‚', 'ä¸­å›½', 'æ¾å±', 'é˜”å¶æ ‘', 'å¤©ç‰›', 'å¤©æ•Œæ˜†è™«',
    'çº¿è™«', 'çœŸèŒ', 'ç®—æ³•', 'é¥æ„ŸæŠ€æœ¯', 'åˆ†å­ç”Ÿç‰©å­¦æŠ€æœ¯', 'å¹´ä»½',
    'ç—…å®³', 'å†œè¯è¯å‰‚', 'ç ”ç©¶æ¨¡å‹ä¸è½¯ä»¶', 'åŸºå› ', 'ä»£è°¢é€šè·¯',
    'ç‰©ç†é˜²æ²»', 'åŒ–å­¦é˜²æ²»', 'è¥æ—é˜²æ²»', 'æ£€ç–«æªæ–½', 'ç”Ÿç†æŒ‡æ ‡',
    'é£é™©è¯„ä¼°', 'æ—©æœŸè¯Šæ–­', 'æ£®æ—ä¿æŠ¤å­¦', 'æ£®æ—æ˜†è™«å­¦', 'æ£®æ—ç—…ç†å­¦',
    'æ—ä¸šæ¤ç‰©æ£€ç–«å­¦', 'åšå£«å­¦ä½è®ºæ–‡', 'å›½å®¶ç§‘æŠ€è¿›æ­¥äºŒç­‰å¥–', 'ç”Ÿæ€æœåŠ¡',
    'å¤šå°ºåº¦ç›‘æµ‹', 'èƒ½é‡ä»£è°¢', 'è¯Šæ–­', 'å¤©æ•Œ',
    'ç§ç¾¤åŠ¨æ€æ¨¡å‹', 'æ¤è¢«æŒ‡æ•°', 'å…‰è°±ç‰¹å¾'
]

_existing_names = {node["node_name"] for node in _CORE_HIGH_LEVEL_NODE_RECORDS}
DEFAULT_HIGH_LEVEL_NODE_RECORDS = [
    *_CORE_HIGH_LEVEL_NODE_RECORDS,
    *[
        {
            "node_name": name,
            "node_type": "generic",
            "description": "é»˜è®¤é«˜çº§èŠ‚ç‚¹"
        }
        for name in _GENERIC_HIGH_LEVEL_NODE_NAMES
        if name not in _existing_names
    ]
]


# ==================== æ•°æ®æ¨¡å‹ ====================
class Triple(BaseModel):
    """ä¸‰å…ƒç»„æ¨¡å‹"""
    id: Optional[int] = None
    head_entity: str
    relation: str
    tail_entity: str


class Node(BaseModel):
    """èŠ‚ç‚¹æ¨¡å‹"""
    name: str


class UpdateNode(BaseModel):
    """æ›´æ–°èŠ‚ç‚¹æ¨¡å‹"""
    old_name: str
    new_name: str


class GraphResponse(BaseModel):
    """å›¾è°±å“åº”æ¨¡å‹"""
    nodes: List[dict]
    links: List[dict]


class ImageAnalysisRequest(BaseModel):
    """å›¾åƒåˆ†æè¯·æ±‚æ¨¡å‹"""
    analyze_type: str = "full"  # full, entity_only, relationship_only
    update_knowledge: bool = True  # æ˜¯å¦è‡ªåŠ¨æ›´æ–°çŸ¥è¯†å›¾è°±
    confidence_threshold: Optional[float] = 0.5


class ImageAnalysisResponse(BaseModel):
    """å›¾åƒåˆ†æå“åº”æ¨¡å‹"""
    analysis_id: str
    image_info: dict
    detected_entities: List[dict]
    relationship_analysis: Optional[dict] = None
    disease_prediction: Optional[dict] = None
    knowledge_update: Optional[dict] = None
    recommendations: List[str]
    analysis_summary: dict


class EntityValidationRequest(BaseModel):
    """å®ä½“éªŒè¯è¯·æ±‚æ¨¡å‹"""
    entities: List[dict]
    validation_type: str = "disease_scenario"  # disease_scenario, relationship_check


class HighLevelNodePayload(BaseModel):
    """é«˜çº§èŠ‚ç‚¹è¯·æ±‚è½½ä½“"""
    node_name: str


# ==================== æ•°æ®åº“æ“ä½œ ====================
@contextmanager
def get_db():
    """æ•°æ®åº“è¿æ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    conn = pymysql.connect(**DB_CONFIG)
    try:
        yield conn
    finally:
        conn.close()


def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    with get_db() as conn:
        cursor = conn.cursor()
        
        # åˆ›å»ºä¸‰å…ƒç»„è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS knowledge_triples (
                id INT AUTO_INCREMENT PRIMARY KEY,
                head_entity VARCHAR(255) NOT NULL,
                relation VARCHAR(100) NOT NULL,
                tail_entity VARCHAR(255) NOT NULL,
                INDEX idx_head_entity (head_entity),
                INDEX idx_tail_entity (tail_entity)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
        """)
        
        # åˆ›å»ºæœ‰æ•ˆå…³ç³»è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS valid_relations (
                id INT AUTO_INCREMENT PRIMARY KEY,
                relation_name VARCHAR(100) UNIQUE NOT NULL
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
        """)
        
        # åˆ›å»ºé«˜çº§èŠ‚ç‚¹ä¸“ç”¨è¡¨
        cursor.execute(f"""
            CREATE TABLE IF NOT EXISTS {HIGH_LEVEL_NODE_TABLE} (
                id INT AUTO_INCREMENT PRIMARY KEY,
                node_name VARCHAR(255) UNIQUE NOT NULL,
                node_type ENUM('core', 'generic') DEFAULT 'generic',
                description VARCHAR(512) DEFAULT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                INDEX idx_node_name (node_name)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
        """)
        
        # åˆ›å»ºå›¾åƒåˆ†æå†å²è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS image_analysis_history (
                id INT AUTO_INCREMENT PRIMARY KEY,
                analysis_id VARCHAR(100) NOT NULL UNIQUE,
                timestamp DATETIME NOT NULL,
                entity_count INT NOT NULL,
                detected_types JSON NOT NULL,
                confidence FLOAT NOT NULL,
                risk_level VARCHAR(20) NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                INDEX idx_timestamp (timestamp),
                INDEX idx_analysis_id (analysis_id)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
        """)
        
        conn.commit()
        logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")


# ==================== é«˜çº§èŠ‚ç‚¹ç®¡ç† ====================
def load_high_level_nodes_from_db() -> set:
    """ä»æ•°æ®åº“åŠ è½½é«˜çº§èŠ‚ç‚¹"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            cursor.execute(f"SELECT node_name FROM {HIGH_LEVEL_NODE_TABLE}")
            nodes = {row["node_name"] for row in cursor.fetchall()}
            logger.info(f"ä»æ•°æ®åº“åŠ è½½äº† {len(nodes)} ä¸ªé«˜çº§èŠ‚ç‚¹")
            return nodes
    except Exception as e:
        logger.error(f"ä»æ•°æ®åº“åŠ è½½é«˜çº§èŠ‚ç‚¹å¤±è´¥: {e}")
        return set()


def save_high_level_nodes_to_db(high_level_nodes: set, replace_all: bool = False):
    """
    ä¿å­˜é«˜çº§èŠ‚ç‚¹åˆ°æ•°æ®åº“
    
    Args:
        high_level_nodes: è¦ä¿å­˜çš„é«˜çº§èŠ‚ç‚¹é›†åˆ
        replace_all: å¦‚æœä¸ºTrueï¼Œå®Œå…¨æ›¿æ¢ï¼ˆåˆ é™¤æ—§çš„ï¼‰ï¼›å¦‚æœä¸ºFalseï¼Œå¢é‡æ›´æ–°ï¼ˆåªæ·»åŠ æ–°çš„ï¼‰
    """
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            if replace_all:
                # å®Œå…¨æ›¿æ¢ï¼šåˆ é™¤æ‰€æœ‰æ—§èŠ‚ç‚¹
                cursor.execute(f"DELETE FROM {HIGH_LEVEL_NODE_TABLE}")
                logger.info("å·²æ¸…é™¤æ‰€æœ‰æ—§çš„é«˜çº§èŠ‚ç‚¹")
            
            # è·å–ç°æœ‰çš„é«˜çº§èŠ‚ç‚¹
            cursor.execute(f"SELECT node_name FROM {HIGH_LEVEL_NODE_TABLE}")
            existing_nodes = {row["node_name"] for row in cursor.fetchall()}
            
            # æ‰¾å‡ºæ–°å¢çš„èŠ‚ç‚¹
            new_nodes = high_level_nodes - existing_nodes
            
            if new_nodes:
                # æ’å…¥æ–°èŠ‚ç‚¹
                cursor.executemany(
                    f"INSERT IGNORE INTO {HIGH_LEVEL_NODE_TABLE} (node_name) VALUES (%s)",
                    [(node,) for node in new_nodes]
                )
                conn.commit()
                logger.info(f"æ–°å¢ {len(new_nodes)} ä¸ªé«˜çº§èŠ‚ç‚¹åˆ°æ•°æ®åº“: {new_nodes}")
            else:
                logger.info("æ²¡æœ‰æ–°å¢çš„é«˜çº§èŠ‚ç‚¹")
            
            # å¦‚æœå®Œå…¨æ›¿æ¢ï¼Œè¿”å›æ–°çš„èŠ‚ç‚¹é›†åˆï¼›å¦åˆ™è¿”å›åˆå¹¶åçš„
            if replace_all:
                return high_level_nodes
            else:
                return existing_nodes | high_level_nodes
    except Exception as e:
        logger.error(f"ä¿å­˜é«˜çº§èŠ‚ç‚¹åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        return high_level_nodes


def get_default_high_level_nodes() -> set:
    """è·å–é»˜è®¤çš„é«˜çº§èŠ‚ç‚¹åç§°é›†åˆ"""
    return {node["node_name"] for node in DEFAULT_HIGH_LEVEL_NODE_RECORDS}


def get_default_high_level_node_records():
    """è¿”å›é»˜è®¤é«˜çº§èŠ‚ç‚¹çš„å®Œæ•´è®°å½•"""
    return DEFAULT_HIGH_LEVEL_NODE_RECORDS


def init_default_high_level_nodes():
    """åˆå§‹åŒ–é»˜è®¤çš„é«˜çº§èŠ‚ç‚¹åˆ°æ•°æ®åº“ï¼ˆå¦‚æœæ•°æ®åº“ä¸ºç©ºï¼‰"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            cursor.execute(f"SELECT COUNT(*) as cnt FROM {HIGH_LEVEL_NODE_TABLE}")
            count = cursor.fetchone()["cnt"]
            
            if count == 0:
                # æ•°æ®åº“ä¸ºç©ºï¼Œåˆå§‹åŒ–é»˜è®¤åˆ—è¡¨
                default_nodes = get_default_high_level_node_records()
                cursor.executemany(
                    f"""
                    INSERT IGNORE INTO {HIGH_LEVEL_NODE_TABLE} (node_name, node_type, description) 
                    VALUES (%s, %s, %s)
                    """,
                    [
                        (node["node_name"], node.get("node_type", "generic"), node.get("description"))
                        for node in default_nodes
                    ]
                )
                conn.commit()
                logger.info(f"åˆå§‹åŒ–äº† {len(default_nodes)} ä¸ªé»˜è®¤é«˜çº§èŠ‚ç‚¹åˆ°æ•°æ®åº“")
                return {node["node_name"] for node in default_nodes}
            else:
                logger.info(f"æ•°æ®åº“ä¸­å·²æœ‰ {count} ä¸ªé«˜çº§èŠ‚ç‚¹ï¼Œè·³è¿‡åˆå§‹åŒ–")
                return load_high_level_nodes_from_db()
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–é»˜è®¤é«˜çº§èŠ‚ç‚¹å¤±è´¥: {e}")
        return get_default_high_level_nodes()


# ==================== APIè·¯ç”± ====================
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–æ•°æ®åº“å’ŒAIæœåŠ¡"""
    init_database()
    
    # åˆå§‹åŒ–AIæœåŠ¡
    from ai_service import init_ai_services
    word2vec_path = os.getenv("WORD2VEC_MODEL_PATH")
    kimi_api_key = os.getenv("MOONSHOT_API_KEY")
    init_ai_services(word2vec_path, kimi_api_key)
    
    # åˆå§‹åŒ–å›¾åƒåˆ†ææœåŠ¡
    from image_service import init_image_services
    from knowledge_updater import init_knowledge_updater
    from multi_entity_analyzer import init_multi_entity_analyzer
    
    init_image_services(DB_CONFIG)
    init_knowledge_updater(DB_CONFIG)
    init_multi_entity_analyzer(DB_CONFIG)
    
    logger.info("åº”ç”¨å¯åŠ¨å®Œæˆ")


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {"message": "æ¾æçº¿è™«ç—…çŸ¥è¯†å›¾è°±ç³»ç»ŸAPI", "version": "1.0.0"}


@app.get("/api/graph", response_model=GraphResponse)
async def get_graph():
    """
    è·å–å®Œæ•´çŸ¥è¯†å›¾è°±
    è¿”å›EChartsæ‰€éœ€çš„nodeså’Œlinksæ ¼å¼
    
    Args:
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜çš„é«˜çº§èŠ‚ç‚¹ï¼ˆé»˜è®¤Trueï¼‰
    """
    try:
        with get_db() as conn:
            cursor = conn.cursor()

            # æŸ¥è¯¢æ‰€æœ‰ä¸‰å…ƒç»„
            cursor.execute("SELECT * FROM knowledge_triples")
            triples = cursor.fetchall()

            # æ„å»ºèŠ‚ç‚¹å’Œè¾¹
            nodes_set = set()
            links = []

            for triple in triples:
                head = triple["head_entity"]
                relation = triple["relation"]
                tail = triple["tail_entity"]

                nodes_set.add(head)
                nodes_set.add(tail)

                links.append({
                    "source": head,
                    "target": tail,
                    "value": relation,
                    "id": triple["id"]
                })

            # ä»æ•°æ®åº“åŠ è½½é«˜çº§èŠ‚ç‚¹ï¼ˆæŒä¹…åŒ–å­˜å‚¨ï¼‰
            high_level_nodes = load_high_level_nodes_from_db()
            
            # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰é«˜çº§èŠ‚ç‚¹ï¼Œåˆå§‹åŒ–é»˜è®¤åˆ—è¡¨
            if not high_level_nodes:
                high_level_nodes = init_default_high_level_nodes()
                # åªä¿ç•™åœ¨å›¾è°±ä¸­å®é™…å­˜åœ¨çš„èŠ‚ç‚¹
                high_level_nodes = high_level_nodes.intersection(nodes_set)
            else:
                logger.info(f"ä»æ•°æ®åº“åŠ è½½é«˜çº§èŠ‚ç‚¹ï¼Œå…± {len(high_level_nodes)} ä¸ª")

            # è½¬æ¢èŠ‚ç‚¹æ ¼å¼ï¼Œæ·»åŠ ç±»åˆ«ä¿¡æ¯
            nodes = []
            for node in nodes_set:
                node_data = {
                    "name": node,
                    "id": node,
                    "category": 1 if node in high_level_nodes else 0  # 1=é«˜çº§èŠ‚ç‚¹ï¼Œ0=æ™®é€šèŠ‚ç‚¹
                }
                nodes.append(node_data)

            return GraphResponse(nodes=nodes, links=links)

    except Exception as e:
        logger.error(f"è·å–å›¾è°±å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å›¾è°±å¤±è´¥: {str(e)}")


@app.delete("/api/node/delete")
async def delete_node(node: Node):
    """
    åˆ é™¤èŠ‚ç‚¹åŠå…¶ç›¸å…³çš„æ‰€æœ‰è¾¹
    """
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # åˆ é™¤åŒ…å«è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰ä¸‰å…ƒç»„
            cursor.execute("""
                DELETE FROM knowledge_triples 
                WHERE head_entity = %s OR tail_entity = %s
            """, (node.name, node.name))
            
            deleted_count = cursor.rowcount
            conn.commit()
            
            logger.info(f"åˆ é™¤èŠ‚ç‚¹ {node.name}, åˆ é™¤äº† {deleted_count} æ¡è®°å½•")
            
            # å¦‚æœåˆ é™¤çš„èŠ‚ç‚¹æ˜¯é«˜çº§èŠ‚ç‚¹ï¼Œä¹Ÿä»é«˜çº§èŠ‚ç‚¹è¡¨ä¸­åˆ é™¤
            try:
                cursor.execute(f"DELETE FROM {HIGH_LEVEL_NODE_TABLE} WHERE node_name = %s", (node.name,))
                conn.commit()
                logger.info(f"å·²ä»é«˜çº§èŠ‚ç‚¹è¡¨ä¸­åˆ é™¤: {node.name}")
            except Exception as e:
                logger.warning(f"ä»é«˜çº§èŠ‚ç‚¹è¡¨åˆ é™¤å¤±è´¥: {e}")
            
            return {"message": f"æˆåŠŸåˆ é™¤èŠ‚ç‚¹ {node.name}", "deleted_count": deleted_count}
            
    except Exception as e:
        logger.error(f"åˆ é™¤èŠ‚ç‚¹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤èŠ‚ç‚¹å¤±è´¥: {str(e)}")


@app.put("/api/node/update")
async def update_node(update: UpdateNode):
    """
    æ›´æ–°èŠ‚ç‚¹åç§°
    """
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # æ›´æ–°å¤´å®ä½“
            cursor.execute("""
                UPDATE knowledge_triples 
                SET head_entity = %s 
                WHERE head_entity = %s
            """, (update.new_name, update.old_name))
            
            # æ›´æ–°å°¾å®ä½“
            cursor.execute("""
                UPDATE knowledge_triples 
                SET tail_entity = %s 
                WHERE tail_entity = %s
            """, (update.new_name, update.old_name))
            
            updated_count = cursor.rowcount
            conn.commit()
            
            logger.info(f"æ›´æ–°èŠ‚ç‚¹ {update.old_name} -> {update.new_name}")
            
            # å¦‚æœæ—§èŠ‚ç‚¹æ˜¯é«˜çº§èŠ‚ç‚¹ï¼Œæ›´æ–°é«˜çº§èŠ‚ç‚¹è¡¨ä¸­çš„åç§°
            try:
                cursor.execute(f"""
                    UPDATE {HIGH_LEVEL_NODE_TABLE} 
                    SET node_name = %s 
                    WHERE node_name = %s
                """, (update.new_name, update.old_name))
                conn.commit()
                if cursor.rowcount > 0:
                    logger.info(f"å·²æ›´æ–°é«˜çº§èŠ‚ç‚¹è¡¨ä¸­çš„èŠ‚ç‚¹åç§°: {update.old_name} -> {update.new_name}")
            except Exception as e:
                logger.warning(f"æ›´æ–°é«˜çº§èŠ‚ç‚¹è¡¨å¤±è´¥: {e}")
            
            return {"message": f"æˆåŠŸæ›´æ–°èŠ‚ç‚¹", "updated_count": updated_count}
            
    except Exception as e:
        logger.error(f"æ›´æ–°èŠ‚ç‚¹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°èŠ‚ç‚¹å¤±è´¥: {str(e)}")


@app.delete("/api/edge/delete/{edge_id}")
async def delete_edge(edge_id: int):
    """
    åˆ é™¤æŒ‡å®šçš„è¾¹(ä¸‰å…ƒç»„)
    """
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            cursor.execute("DELETE FROM knowledge_triples WHERE id = %s", (edge_id,))
            
            if cursor.rowcount == 0:
                raise HTTPException(status_code=404, detail="è¾¹ä¸å­˜åœ¨")
            
            conn.commit()
            
            logger.info(f"åˆ é™¤è¾¹ ID: {edge_id}")
            return {"message": f"æˆåŠŸåˆ é™¤è¾¹"}
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤è¾¹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤è¾¹å¤±è´¥: {str(e)}")


@app.put("/api/edge/update")
async def update_edge(triple: Triple):
    """
    æ›´æ–°è¾¹(ä¸‰å…ƒç»„)
    """
    try:
        if triple.id is None:
            raise HTTPException(status_code=400, detail="éœ€è¦æä¾›è¾¹çš„ID")
        
        with get_db() as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                UPDATE knowledge_triples 
                SET head_entity = %s, relation = %s, tail_entity = %s
                WHERE id = %s
            """, (triple.head_entity, triple.relation, triple.tail_entity, triple.id))
            
            if cursor.rowcount == 0:
                raise HTTPException(status_code=404, detail="è¾¹ä¸å­˜åœ¨")
            
            conn.commit()
            
            logger.info(f"æ›´æ–°è¾¹ ID: {triple.id}")
            return {"message": "æˆåŠŸæ›´æ–°è¾¹"}
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°è¾¹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°è¾¹å¤±è´¥: {str(e)}")


@app.get("/api/relations")
async def get_relations():
    """
    è·å–æ‰€æœ‰æœ‰æ•ˆå…³ç³»åˆ—è¡¨
    """
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT relation_name FROM valid_relations")
            relations = [row["relation_name"] for row in cursor.fetchall()]
            return {"relations": relations}
            
    except Exception as e:
        logger.error(f"è·å–å…³ç³»åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å…³ç³»åˆ—è¡¨å¤±è´¥: {str(e)}")


@app.get("/api/node/similar/{entity_name}")
async def get_similar_entities(entity_name: str, topn: int = 10):
    """
    è·å–ç›¸ä¼¼å®ä½“åˆ—è¡¨ï¼ˆæ–°å¢èŠ‚ç‚¹çš„ç¬¬ä¸€æ­¥ï¼‰
    
    Args:
        entity_name: è¾“å…¥çš„å®ä½“åç§°
        topn: è¿”å›å‰Nä¸ªç›¸ä¼¼å®ä½“ï¼ˆé»˜è®¤10ä¸ªï¼‰
    
    Returns:
        ç›¸ä¼¼å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«ï¼šåç§°ã€ç›¸ä¼¼åº¦ã€æ˜¯å¦åœ¨å›¾è°±ä¸­
    """
    from ai_service import get_word2vec_service
    
    entity_name = entity_name.strip()
    
    if not entity_name:
        raise HTTPException(status_code=400, detail="å®ä½“åç§°ä¸èƒ½ä¸ºç©º")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # æ£€æŸ¥å®ä½“æ˜¯å¦å·²å­˜åœ¨
            cursor.execute("""
                SELECT COUNT(*) as cnt FROM knowledge_triples 
                WHERE head_entity = %s OR tail_entity = %s
            """, (entity_name, entity_name))
            
            if cursor.fetchone()["cnt"] > 0:
                raise HTTPException(status_code=400, detail=f"å®ä½“ '{entity_name}' å·²å­˜åœ¨äºå›¾è°±ä¸­")
            
            # ä»æ•°æ®åº“è·å–æ‰€æœ‰å·²å­˜åœ¨çš„å®ä½“
            cursor.execute("""
                SELECT DISTINCT head_entity as entity FROM knowledge_triples
                UNION
                SELECT DISTINCT tail_entity as entity FROM knowledge_triples
            """)
            
            existing_entities = [row["entity"] for row in cursor.fetchall()]
            
            if not existing_entities:
                raise HTTPException(status_code=404, detail="å›¾è°±ä¸­æš‚æ— å®ä½“ï¼Œæ— æ³•è®¡ç®—ç›¸ä¼¼åº¦")
            
            logger.info(f"ä»æ•°æ®åº“è·å–äº† {len(existing_entities)} ä¸ªå·²æœ‰å®ä½“")
            
            # ä½¿ç”¨Word2Vecè®¡ç®—è¾“å…¥è¯ä¸æ‰€æœ‰å®ä½“çš„ç›¸ä¼¼åº¦
            word2vec = get_word2vec_service()
            similar_words = word2vec.calculate_similarity_with_candidates(entity_name, existing_entities)
            
            if not similar_words:
                raise HTTPException(status_code=404, detail="æœªèƒ½è®¡ç®—ç›¸ä¼¼åº¦")
            
            # åªå–å‰topnä¸ªï¼ˆå·²ç»æŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
            similar_words = similar_words[:topn]
            
            # æ„å»ºè¿”å›ç»“æœï¼ˆè¿™äº›éƒ½æ˜¯å›¾è°±å†…çš„å®ä½“ï¼‰
            result = []
            for word, similarity in similar_words:
                entity_data = {
                    "entity": word,
                    "similarity": float(similarity),
                    "in_graph": True  # éƒ½æ˜¯ä»æ•°æ®åº“æŸ¥å‡ºæ¥çš„ï¼Œå¿…ç„¶åœ¨å›¾è°±ä¸­
                }
                result.append(entity_data)
            
            if not result:
                raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°ç›¸ä¼¼å®ä½“")
            
            logger.info(f"è®¡ç®—å®Œæˆï¼Œè¿”å› {len(result)} ä¸ªç›¸ä¼¼å®ä½“ï¼ˆç›¸ä¼¼åº¦èŒƒå›´: {result[0]['similarity']:.4f} ~ {result[-1]['similarity']:.4f}ï¼‰")
            
            return {
                "input": entity_name,
                "similar_entities": result,
                "stats": {
                    "total_entities_in_graph": len(existing_entities),
                    "calculated_count": len(existing_entities),
                    "returned_count": len(result)
                }
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŸ¥è¯¢ç›¸ä¼¼å®ä½“å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢ç›¸ä¼¼å®ä½“å¤±è´¥: {str(e)}")


class GenerateTriples(BaseModel):
    """ç”Ÿæˆå€™é€‰ä¸‰å…ƒç»„çš„è¯·æ±‚"""
    entity_name: str
    similar_entity: str


class SelectedTriple(BaseModel):
    """ç”¨æˆ·é€‰æ‹©çš„ä¸‰å…ƒç»„"""
    entity_name: str
    similar_entity: str
    selected_triple: dict  # {head_entity, relation, tail_entity}


@app.post("/api/node/generate-triples")
async def generate_candidate_triples(data: GenerateTriples):
    """
    ç”Ÿæˆå€™é€‰ä¸‰å…ƒç»„ï¼ˆæ–°çš„ç¬¬äºŒæ­¥ï¼šåŸºäºé€‰æ‹©çš„ç›¸ä¼¼è¯ç”Ÿæˆå¤šä¸ªå€™é€‰ï¼‰
    
    æ­¥éª¤ï¼š
    1. ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„ç›¸ä¼¼è¯B
    2. æŸ¥è¯¢æ•°æ®åº“ï¼Œæ‰¾åˆ°ä¸Bç›¸å…³çš„**æ‰€æœ‰**å®ä½“C
    3. ä½¿ç”¨AIä¸ºæ¯ä¸ª(A, C)å¯¹æ¨ç†å…³ç³»
    4. è¿”å›æ‰€æœ‰å€™é€‰ä¸‰å…ƒç»„ä¾›ç”¨æˆ·é€‰æ‹©
    """
    from ai_service import get_kimi_service
    
    entity_a = data.entity_name.strip()
    entity_b = data.similar_entity.strip()
    
    if not entity_a or not entity_b:
        raise HTTPException(status_code=400, detail="å®ä½“åç§°ä¸èƒ½ä¸ºç©º")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # æ£€æŸ¥å®ä½“Aæ˜¯å¦å·²å­˜åœ¨
            cursor.execute("""
                SELECT COUNT(*) as cnt FROM knowledge_triples 
                WHERE head_entity = %s OR tail_entity = %s
            """, (entity_a, entity_a))
            
            if cursor.fetchone()["cnt"] > 0:
                raise HTTPException(status_code=400, detail=f"å®ä½“ '{entity_a}' å·²å­˜åœ¨äºå›¾è°±ä¸­")
            
            logger.info(f"æ­¥éª¤1: ç”¨æˆ·é€‰æ‹©ç›¸ä¼¼è¯ {entity_a} -> {entity_b}")
            
            # æ­¥éª¤2: æŸ¥è¯¢ä¸Bç›¸å…³çš„**æ‰€æœ‰**å®ä½“ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
            cursor.execute("""
                SELECT DISTINCT 
                    CASE 
                        WHEN head_entity = %s THEN tail_entity 
                        ELSE head_entity 
                    END as related_entity
                FROM knowledge_triples 
                WHERE head_entity = %s OR tail_entity = %s
            """, (entity_b, entity_b, entity_b))
            
            related_entities = [row["related_entity"] for row in cursor.fetchall()]
            
            if not related_entities:
                raise HTTPException(
                    status_code=404, 
                    detail=f"ç›¸ä¼¼å®ä½“ '{entity_b}' ä¸åœ¨å›¾è°±ä¸­ï¼Œæ— æ³•å»ºç«‹å…³è”"
                )
            
            logger.info(f"æ­¥éª¤2å®Œæˆ: æ‰¾åˆ° {len(related_entities)} ä¸ªå…³è”å®ä½“")
            
            # æ­¥éª¤3: è·å–æœ‰æ•ˆå…³ç³»åˆ—è¡¨
            cursor.execute("SELECT relation_name FROM valid_relations")
            valid_relations = [row["relation_name"] for row in cursor.fetchall()]
            
            if not valid_relations:
                raise HTTPException(status_code=500, detail="ç³»ç»Ÿä¸­æ²¡æœ‰é…ç½®æœ‰æ•ˆå…³ç³»")
            
            # æ­¥éª¤4: ä½¿ç”¨AIä¸ºæ¯ä¸ª(A, C)å¯¹æ¨ç†å…³ç³»
            kimi = get_kimi_service()
            candidate_triples = []
            
            for entity_c in related_entities:
                inferred_relation = kimi.infer_relation(entity_a, entity_c, valid_relations)
                candidate_triples.append({
                    "head_entity": entity_a,
                    "relation": inferred_relation,
                    "tail_entity": entity_c
                })
                logger.info(f"ç”Ÿæˆå€™é€‰: {entity_a} --[{inferred_relation}]--> {entity_c}")
            
            return {
                "input_entity": entity_a,
                "similar_entity": entity_b,
                "candidate_triples": candidate_triples,
                "total_candidates": len(candidate_triples)
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç”Ÿæˆå€™é€‰ä¸‰å…ƒç»„å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå€™é€‰ä¸‰å…ƒç»„å¤±è´¥: {str(e)}")


@app.post("/api/node/add")
async def add_node_with_selected_triple(data: SelectedTriple):
    """
    æ·»åŠ èŠ‚ç‚¹ï¼ˆæ–°çš„ç¬¬ä¸‰æ­¥ï¼šç”¨æˆ·é€‰æ‹©ä¸‰å…ƒç»„åæ’å…¥æ•°æ®åº“ï¼‰
    
    æ­¥éª¤ï¼š
    1. éªŒè¯é€‰æ‹©çš„ä¸‰å…ƒç»„
    2. æ’å…¥æ•°æ®åº“
    """
    entity_a = data.entity_name.strip()
    entity_b = data.similar_entity.strip()
    triple = data.selected_triple
    
    if not entity_a or not triple:
        raise HTTPException(status_code=400, detail="å‚æ•°ä¸å®Œæ•´")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # å†æ¬¡æ£€æŸ¥å®ä½“Aæ˜¯å¦å·²å­˜åœ¨
            cursor.execute("""
                SELECT COUNT(*) as cnt FROM knowledge_triples 
                WHERE head_entity = %s OR tail_entity = %s
            """, (entity_a, entity_a))
            
            if cursor.fetchone()["cnt"] > 0:
                raise HTTPException(status_code=400, detail=f"å®ä½“ '{entity_a}' å·²å­˜åœ¨äºå›¾è°±ä¸­")
            
            # æ’å…¥é€‰æ‹©çš„ä¸‰å…ƒç»„
            cursor.execute("""
                INSERT INTO knowledge_triples (head_entity, relation, tail_entity)
                VALUES (%s, %s, %s)
            """, (triple["head_entity"], triple["relation"], triple["tail_entity"]))
            
            conn.commit()
            triple_id = cursor.lastrowid
            
            logger.info(f"æˆåŠŸæ·»åŠ ä¸‰å…ƒç»„: {triple['head_entity']} --[{triple['relation']}]--> {triple['tail_entity']}")
            
            return {
                "message": "æˆåŠŸæ·»åŠ æ–°å®ä½“",
                "triple": {
                    "id": triple_id,
                    "head_entity": triple["head_entity"],
                    "relation": triple["relation"],
                    "tail_entity": triple["tail_entity"]
                },
                "inference_path": {
                    "input": entity_a,
                    "similar_entity": entity_b
                }
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ™ºèƒ½æ·»åŠ èŠ‚ç‚¹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ™ºèƒ½æ·»åŠ èŠ‚ç‚¹å¤±è´¥: {str(e)}")


# ==================== å›¾åƒåˆ†æAPI ====================
@app.post("/api/image/analyze")
async def analyze_image(
    file: UploadFile = File(...),
    analyze_type: str = Form("full"),
    update_knowledge: bool = Form(True),
    confidence_threshold: float = Form(0.5)
):
    """
    å›¾åƒåˆ†æAPI - è¯†åˆ«æ¾æçº¿è™«ç—…ç›¸å…³å®ä½“å¹¶è¿›è¡Œé¢„æµ‹åˆ†æ
    
    Args:
        file: ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶
        analyze_type: åˆ†æç±»å‹ (full/entity_only/relationship_only)
        update_knowledge: æ˜¯å¦è‡ªåŠ¨æ›´æ–°çŸ¥è¯†å›¾è°±
        confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
    
    Returns:
        å®Œæ•´çš„åˆ†æç»“æœ
    """
    if not file.content_type or not file.content_type.startswith('image/'):
        raise HTTPException(status_code=400, detail="è¯·ä¸Šä¼ å›¾åƒæ–‡ä»¶")
    
    try:
        # 1. è¯»å–å›¾åƒæ•°æ®
        image_data = await file.read()
        
        # 2. å›¾åƒåˆ†æ - å®ä½“è¯†åˆ«ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œå¤±è´¥åˆ™ä½¿ç”¨äº‘ç«¯Kimiï¼‰
        analysis_result = None
        service_used = None
        
        # 2.1 å°è¯•ä½¿ç”¨æœ¬åœ° YOLO æ¨¡å‹
        try:
            logger.info("å°è¯•ä½¿ç”¨æœ¬åœ° YOLO æ¨¡å‹è¿›è¡Œå›¾åƒè¯†åˆ«...")
            from local_yolo_image_service import LocalYOLOImageAnalysisService
            
            # åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹æœåŠ¡
            local_service = LocalYOLOImageAnalysisService(model_path="yolov8m.pt")
            
            # ä½¿ç”¨æœ¬åœ°æœåŠ¡åˆ†æå›¾åƒ
            analysis_result = await local_service.analyze_image(image_data)
            service_used = "æœ¬åœ°YOLOæ¨¡å‹"
            logger.info(f"âœ… æœ¬åœ°æ¨¡å‹è¯†åˆ«æˆåŠŸ: {len(analysis_result.get('detected_entities', []))} ä¸ªå®ä½“")
            
        except Exception as local_error:
            logger.warning(f"âš ï¸ æœ¬åœ°æ¨¡å‹è¯†åˆ«å¤±è´¥: {local_error}")
            
            # 2.2 å›é€€åˆ°äº‘ç«¯ Kimi æ¨¡å‹
            try:
                logger.info("å›é€€åˆ°äº‘ç«¯ Kimi æ¨¡å‹...")
                from vision_ai_image_service import VisionAIImageAnalysisService
                
                # åˆå§‹åŒ– Kimi æœåŠ¡
                kimi_service = VisionAIImageAnalysisService()
                
                # ä½¿ç”¨ Kimi æœåŠ¡åˆ†æå›¾åƒ
                analysis_result = await kimi_service.analyze_image(image_data)
                service_used = "äº‘ç«¯Kimiæ¨¡å‹"
                logger.info(f"âœ… Kimi æ¨¡å‹è¯†åˆ«æˆåŠŸ: {len(analysis_result.get('detected_entities', []))} ä¸ªå®ä½“")
                
            except Exception as kimi_error:
                logger.error(f"âŒ Kimi æ¨¡å‹ä¹Ÿå¤±è´¥: {kimi_error}")
                
                # 2.3 æœ€ç»ˆå›é€€ï¼šä½¿ç”¨é€šç”¨å›¾åƒæœåŠ¡
                try:
                    logger.info("å°è¯•ä½¿ç”¨é€šç”¨å›¾åƒæœåŠ¡...")
                    from get_image_analysis_service import get_image_analysis_service
                    image_service = get_image_analysis_service()
                    
                    analysis_result = await image_service.analyze_image(image_data)
                    service_used = "é€šç”¨å›¾åƒæœåŠ¡"
                    logger.info(f"âœ… é€šç”¨æœåŠ¡è¯†åˆ«æˆåŠŸ: {len(analysis_result.get('detected_entities', []))} ä¸ªå®ä½“")
                    
                except Exception as fallback_error:
                    logger.error(f"âŒ æ‰€æœ‰å›¾åƒæœåŠ¡å‡ä¸å¯ç”¨: {fallback_error}")
                    
                    # 2.4 æœ€ç»ˆå›é€€ï¼šè¿”å›æ¨¡æ‹Ÿæ•°æ®ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
                    logger.warning("âš ï¸ è¿”å›æ¨¡æ‹Ÿæ•°æ®ä»¥ç»´æŒç³»ç»Ÿè¿è¡Œ")
                    analysis_result = {
                        "image_info": {"size": [800, 600], "channels": 3},
                        "detected_entities": [
                            {
                                "type": "insect",
                                "name": "ç–‘ä¼¼æ¾å¢¨å¤©ç‰›",
                                "confidence": 0.85,
                                "similarity": 0.8,
                                "features": {"color": "é»‘è‰²"},
                                "bbox": [100, 150, 80, 120],
                                "matched_kb_entity": "æ¾å¢¨å¤©ç‰›"
                            },
                            {
                                "type": "disease_symptom",
                                "name": "ç–‘ä¼¼æ¾é’ˆå‘é»„",
                                "confidence": 0.92,
                                "similarity": 0.7,
                                "features": {"color": "é»„è‰²"},
                                "bbox": [200, 100, 150, 200],
                                "matched_kb_entity": None
                            },
                            {
                                "type": "tree",
                                "name": "ç–‘ä¼¼é©¬å°¾æ¾",
                                "confidence": 0.78,
                                "similarity": 0.6,
                                "features": {"bark": "çº¢è¤è‰²"},
                                "bbox": [0, 0, 800, 600],
                                "matched_kb_entity": "é©¬å°¾æ¾"
                            }
                        ],
                        "analysis_summary": {"total_entities": 3, "matched_entities": 2, "avg_confidence": 0.85}
                    }
                    service_used = "æ¨¡æ‹Ÿæ•°æ®ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰"
        
        # ç¡®ä¿åˆ†æç»“æœå­˜åœ¨
        if analysis_result is None:
            raise HTTPException(status_code=500, detail="å›¾åƒåˆ†æå¤±è´¥ï¼šæ‰€æœ‰æœåŠ¡å‡ä¸å¯ç”¨")
        
        logger.info(f"ğŸ“Š å›¾åƒåˆ†æå®Œæˆ - ä½¿ç”¨æœåŠ¡: {service_used}")
        
        # 3. è¿‡æ»¤ä½ç½®ä¿¡åº¦å®ä½“
        logger.info(f"è¿‡æ»¤å‰å®ä½“æ•°é‡: {len(analysis_result['detected_entities'])}, é˜ˆå€¼: {confidence_threshold}")
        detected_entities = [
            entity for entity in analysis_result["detected_entities"]
            if entity["confidence"] >= confidence_threshold
        ]
        logger.info(f"è¿‡æ»¤åå®ä½“æ•°é‡: {len(detected_entities)}")
        
        response_data = {
            "analysis_id": f"img_analysis_{int(time.time())}",
            "image_info": analysis_result["image_info"],
            "detected_entities": detected_entities,
            "recommendations": [],
            "analysis_summary": analysis_result["analysis_summary"]
        }
        
        # 4. å…³ç³»åˆ†æï¼ˆå¦‚æœè¯·æ±‚ä¸”æœ‰å¤šä¸ªå®ä½“ï¼‰
        if analyze_type in ["full", "relationship_only"] and len(detected_entities) > 1:
            try:
                from multi_entity_analyzer import get_multi_entity_analyzer
                multi_analyzer = get_multi_entity_analyzer()
                
                relationship_result = await multi_analyzer.analyze_entity_relationships(detected_entities)
                response_data["relationship_analysis"] = relationship_result
                response_data["recommendations"].extend(relationship_result["recommendations"])
            except ImportError:
                logger.warning("å¤šå®ä½“åˆ†ææœåŠ¡ä¸å¯ç”¨")
        
        # 5. ç–¾ç—…é¢„æµ‹åˆ†æ
        if analyze_type == "full" and detected_entities:
            try:
                from image_service import get_knowledge_inference_service
                inference_service = get_knowledge_inference_service()
                disease_prediction = await inference_service.analyze_disease_prediction(detected_entities)
                response_data["disease_prediction"] = disease_prediction
                
                if disease_prediction.get("recommended_actions"):
                    response_data["recommendations"].extend([
                        f"é˜²æ²»å»ºè®®: {treatment['treatment']}" 
                        for treatment in disease_prediction["recommended_actions"].get("treatments", [])
                    ])
            except (ImportError, Exception) as e:
                logger.warning(f"ç–¾ç—…é¢„æµ‹æœåŠ¡ä¸å¯ç”¨: {e}")
        
        # 6. çŸ¥è¯†å›¾è°±æ›´æ–°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if update_knowledge and detected_entities:
            try:
                from knowledge_updater import get_knowledge_updater
                updater = get_knowledge_updater()
                
                update_stats = await updater.process_image_analysis_result({
                    "detected_entities": detected_entities
                })
                response_data["knowledge_update"] = update_stats
                
                if update_stats["new_entities_added"] > 0 or update_stats["new_relations_added"] > 0:
                    response_data["recommendations"].append(
                        f"çŸ¥è¯†å›¾è°±å·²æ›´æ–°: æ–°å¢{update_stats['new_entities_added']}ä¸ªå®ä½“, {update_stats['new_relations_added']}ä¸ªå…³ç³»"
                    )
            except (ImportError, Exception) as e:
                logger.warning(f"çŸ¥è¯†å›¾è°±æ›´æ–°æœåŠ¡ä¸å¯ç”¨: {e}")
        
        # 7. ç”Ÿæˆæ€»ç»“å»ºè®®
        if not response_data["recommendations"]:
            response_data["recommendations"] = ["æœªå‘ç°æ˜æ˜¾çš„æ¾æçº¿è™«ç—…é£é™©ï¼Œå»ºè®®ç»§ç»­ç›‘æµ‹"]
        
        # è®°å½•åˆ†æç»“æœåˆ°å†å²è¡¨
        try:
            with get_db() as conn:
                cursor = conn.cursor()
                
                # å‡†å¤‡å†å²è®°å½•æ•°æ®
                analysis_id = response_data["analysis_id"]
                timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
                entity_count = len(detected_entities)
                
                # è·å–æ£€æµ‹åˆ°çš„å®ä½“ç±»å‹
                detected_types = list(set(entity["type"] for entity in detected_entities))
                
                # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
                avg_confidence = sum(entity["confidence"] for entity in detected_entities) / len(detected_entities) if detected_entities else 0
                # ä¿ç•™ä¸€ä½å°æ•°
                avg_confidence = round(avg_confidence, 1)
                
                # ç¡®å®šé£é™©ç­‰çº§
                if avg_confidence >= 0.8:
                    risk_level = "é«˜é£é™©"
                elif avg_confidence >= 0.6:
                    risk_level = "ä¸­é£é™©"
                else:
                    risk_level = "ä½é£é™©"
                
                # æ’å…¥å†å²è®°å½•
                cursor.execute("""
                    INSERT INTO image_analysis_history 
                    (analysis_id, timestamp, entity_count, detected_types, confidence, risk_level)
                    VALUES (%s, %s, %s, %s, %s, %s)
                """, (analysis_id, timestamp, entity_count, json.dumps(detected_types), avg_confidence, risk_level))
                
                conn.commit()
                logger.info(f"åˆ†æå†å²è®°å½•å·²ä¿å­˜: {analysis_id}")
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†æå†å²è®°å½•å¤±è´¥: {e}")
        
        # è®°å½•åˆ†æç»“æœ
        entity_names = [entity["name"] for entity in detected_entities]
        logger.info(f"å›¾åƒåˆ†æå®Œæˆ: æ£€æµ‹{len(detected_entities)}ä¸ªå®ä½“ {entity_names}")
        return response_data
        
    except Exception as e:
        logger.error(f"å›¾åƒåˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å›¾åƒåˆ†æå¤±è´¥: {str(e)}")


@app.post("/api/entities/validate")
async def validate_entity_combinations(request: EntityValidationRequest):
    """
    éªŒè¯å®ä½“ç»„åˆçš„åˆç†æ€§
    
    Args:
        request: åŒ…å«å®ä½“åˆ—è¡¨å’ŒéªŒè¯ç±»å‹çš„è¯·æ±‚
    
    Returns:
        éªŒè¯ç»“æœå’Œå»ºè®®
    """
    try:
        if request.validation_type == "disease_scenario":
            try:
                from multi_entity_analyzer import get_multi_entity_analyzer
                analyzer = get_multi_entity_analyzer()
                
                validation_result = await analyzer.analyze_entity_relationships(request.entities)
                
                return {
                    "validation_type": request.validation_type,
                    "entities": request.entities,
                    "validation_result": validation_result,
                    "is_valid": validation_result["relationship_confidence"] > 0.5,
                    "confidence": validation_result["relationship_confidence"],
                    "recommendations": validation_result["recommendations"]
                }
            except ImportError:
                return {
                    "validation_type": request.validation_type,
                    "entities": request.entities,
                    "is_valid": False,
                    "confidence": 0.0,
                    "recommendations": ["å®ä½“éªŒè¯æœåŠ¡ä¸å¯ç”¨"]
                }
        elif request.validation_type == "relationship_check":
            # ç®€å•çš„å…³ç³»æ£€æŸ¥
            with get_db() as conn:
                cursor = conn.cursor()
                
                entity_names = [entity.get("matched_kb_entity") or entity["name"] for entity in request.entities]
                
                relationships = []
                for i, entity_a in enumerate(entity_names):
                    for entity_b in entity_names[i+1:]:
                        cursor.execute("""
                            SELECT head_entity, relation, tail_entity FROM knowledge_triples 
                            WHERE (head_entity = %s AND tail_entity = %s) 
                               OR (head_entity = %s AND tail_entity = %s)
                        """, (entity_a, entity_b, entity_b, entity_a))
                        
                        relationships.extend(cursor.fetchall())
                
                return {
                    "validation_type": request.validation_type,
                    "entities": request.entities,
                    "existing_relationships": relationships,
                    "relationship_count": len(relationships),
                    "is_valid": len(relationships) > 0
                }
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„éªŒè¯ç±»å‹")
            
    except Exception as e:
        logger.error(f"å®ä½“éªŒè¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å®ä½“éªŒè¯å¤±è´¥: {str(e)}")


@app.get("/api/knowledge/update-suggestions")
async def get_knowledge_update_suggestions(entity_names: str = None):
    """
    è·å–çŸ¥è¯†å›¾è°±æ›´æ–°å»ºè®®
    
    Args:
        entity_names: é€—å·åˆ†éš”çš„å®ä½“åç§°ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æ›´æ–°å»ºè®®åˆ—è¡¨
    """
    try:
        try:
            from knowledge_updater import get_knowledge_updater
            updater = get_knowledge_updater()
            
            if entity_names:
                # åŸºäºç‰¹å®šå®ä½“ç”Ÿæˆå»ºè®®
                names = [name.strip() for name in entity_names.split(',')]
                
                # æ¨¡æ‹Ÿå®ä½“æ•°æ®ç»“æ„
                mock_entities = []
                for name in names:
                    mock_entities.append({
                        "name": name,
                        "type": "unknown", 
                        "confidence": 0.8,
                        "similarity": 0.3,  # å‡è®¾ä½ç›¸ä¼¼åº¦
                        "features": {}
                    })
                
                suggestions = await updater.get_knowledge_update_suggestions(mock_entities)
            else:
                # è¿”å›é€šç”¨å»ºè®®
                suggestions = [
                    {
                        "type": "general",
                        "priority": "low",
                        "reason": "å®šæœŸæ£€æŸ¥çŸ¥è¯†å›¾è°±å®Œæ•´æ€§",
                        "action": "å»ºè®®å®šæœŸä¸Šä¼ æ–°çš„å›¾åƒè¿›è¡Œåˆ†æï¼Œä»¥å‘ç°æ–°çš„å®ä½“å’Œå…³ç³»"
                    }
                ]
        except ImportError:
            suggestions = [
                {
                    "type": "service_unavailable",
                    "priority": "info",
                    "reason": "çŸ¥è¯†æ›´æ–°æœåŠ¡ä¸å¯ç”¨",
                    "action": "è¯·æ£€æŸ¥æœåŠ¡é…ç½®"
                }
            ]
        
        return {
            "suggestions": suggestions,
            "total_count": len(suggestions)
        }
        
    except Exception as e:
        logger.error(f"è·å–æ›´æ–°å»ºè®®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ›´æ–°å»ºè®®å¤±è´¥: {str(e)}")


@app.post("/api/graph/add-high-level-node")
async def add_high_level_node(
    node_name: Optional[str] = Query(
        default=None,
        description="è¦æ ‡è®°ä¸ºé«˜çº§èŠ‚ç‚¹çš„èŠ‚ç‚¹åç§°",
        alias="node_name"
    ),
    payload: Optional[HighLevelNodePayload] = Body(default=None)
):
    """
    æ‰‹åŠ¨æ·»åŠ é«˜çº§èŠ‚ç‚¹
    å°†æ•°æ®åº“ä¸­å·²æœ‰çš„èŠ‚ç‚¹æ ‡è®°ä¸ºé«˜çº§èŠ‚ç‚¹
    
    Args:
        node_name: èŠ‚ç‚¹åç§°
    """
    resolved_name = node_name or (payload.node_name if payload else None)
    if not resolved_name:
        raise HTTPException(status_code=400, detail="èŠ‚ç‚¹åç§°ä¸èƒ½ä¸ºç©º")
    
    node_name = resolved_name.strip()
    if not node_name:
        raise HTTPException(status_code=400, detail="èŠ‚ç‚¹åç§°ä¸èƒ½ä¸ºç©º")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨äºçŸ¥è¯†å›¾è°±ä¸­
            cursor.execute("""
                SELECT COUNT(*) as cnt FROM knowledge_triples 
                WHERE head_entity = %s OR tail_entity = %s
            """, (node_name, node_name))
            
            if cursor.fetchone()["cnt"] == 0:
                raise HTTPException(status_code=404, detail=f"èŠ‚ç‚¹ '{node_name}' ä¸å­˜åœ¨äºçŸ¥è¯†å›¾è°±ä¸­")
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯é«˜çº§èŠ‚ç‚¹
            cursor.execute(f"SELECT COUNT(*) as cnt FROM {HIGH_LEVEL_NODE_TABLE} WHERE node_name = %s", (node_name,))
            if cursor.fetchone()["cnt"] > 0:
                raise HTTPException(status_code=400, detail=f"èŠ‚ç‚¹ '{node_name}' å·²ç»æ˜¯é«˜çº§èŠ‚ç‚¹")
            
            # æ·»åŠ åˆ°é«˜çº§èŠ‚ç‚¹è¡¨
            cursor.execute(
                f"INSERT INTO {HIGH_LEVEL_NODE_TABLE} (node_name, node_type) VALUES (%s, %s)",
                (node_name, "generic")
            )
            conn.commit()
            
            logger.info(f"æˆåŠŸæ·»åŠ é«˜çº§èŠ‚ç‚¹: {node_name}")
            
            return {
                "message": f"æˆåŠŸå°†èŠ‚ç‚¹ '{node_name}' æ ‡è®°ä¸ºé«˜çº§èŠ‚ç‚¹",
                "node_name": node_name
            }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ·»åŠ é«˜çº§èŠ‚ç‚¹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ·»åŠ é«˜çº§èŠ‚ç‚¹å¤±è´¥: {str(e)}")


@app.delete("/api/graph/remove-high-level-node")
async def remove_high_level_node(
    node_name: Optional[str] = Query(
        default=None,
        description="è¦ç§»é™¤çš„é«˜çº§èŠ‚ç‚¹åç§°",
        alias="node_name"
    ),
    payload: Optional[HighLevelNodePayload] = Body(default=None)
):
    """
    ç§»é™¤é«˜çº§èŠ‚ç‚¹æ ‡è®°
    å°†èŠ‚ç‚¹ä»é«˜çº§èŠ‚ç‚¹åˆ—è¡¨ä¸­ç§»é™¤ï¼ˆä½†ä¸ä¼šåˆ é™¤èŠ‚ç‚¹æœ¬èº«ï¼‰
    
    Args:
        node_name: èŠ‚ç‚¹åç§°
    """
    resolved_name = node_name or (payload.node_name if payload else None)
    if not resolved_name:
        raise HTTPException(status_code=400, detail="èŠ‚ç‚¹åç§°ä¸èƒ½ä¸ºç©º")
    
    node_name = resolved_name.strip()
    if not node_name:
        raise HTTPException(status_code=400, detail="èŠ‚ç‚¹åç§°ä¸èƒ½ä¸ºç©º")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é«˜çº§èŠ‚ç‚¹
            cursor.execute(f"SELECT COUNT(*) as cnt FROM {HIGH_LEVEL_NODE_TABLE} WHERE node_name = %s", (node_name,))
            if cursor.fetchone()["cnt"] == 0:
                raise HTTPException(status_code=404, detail=f"èŠ‚ç‚¹ '{node_name}' ä¸æ˜¯é«˜çº§èŠ‚ç‚¹")
            
            # ä»é«˜çº§èŠ‚ç‚¹è¡¨åˆ é™¤
            cursor.execute(f"DELETE FROM {HIGH_LEVEL_NODE_TABLE} WHERE node_name = %s", (node_name,))
            conn.commit()
            
            logger.info(f"æˆåŠŸç§»é™¤é«˜çº§èŠ‚ç‚¹æ ‡è®°: {node_name}")
            
            return {
                "message": f"æˆåŠŸç§»é™¤èŠ‚ç‚¹ '{node_name}' çš„é«˜çº§èŠ‚ç‚¹æ ‡è®°",
                "node_name": node_name
            }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç§»é™¤é«˜çº§èŠ‚ç‚¹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç§»é™¤é«˜çº§èŠ‚ç‚¹å¤±è´¥: {str(e)}")


@app.get("/api/image/analysis-history")
async def get_analysis_history(limit: int = 10):
    """
    è·å–å›¾åƒåˆ†æå†å²
    
    Args:
        limit: è¿”å›è®°å½•æ•°é‡é™åˆ¶
    
    Returns:
        åˆ†æå†å²åˆ—è¡¨
    """
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # æŸ¥è¯¢æœ€æ–°çš„åˆ†æå†å²è®°å½•
            cursor.execute("""
                SELECT 
                    analysis_id as id,
                    timestamp,
                    entity_count,
                    detected_types,
                    confidence,
                    risk_level
                FROM image_analysis_history 
                ORDER BY timestamp DESC 
                LIMIT %s
            """, (limit,))
            
            history_records = []
            for record in cursor.fetchall():
                # è§£æJSONå­—æ®µ
                detected_types = json.loads(record["detected_types"]) if isinstance(record["detected_types"], str) else record["detected_types"]
                
                history_records.append({
                    "id": record["id"],
                    "timestamp": record["timestamp"].strftime('%Y-%m-%d %H:%M:%S') if hasattr(record["timestamp"], 'strftime') else record["timestamp"],
                    "entity_count": record["entity_count"],
                    "detected_types": detected_types,
                    "confidence": round(float(record["confidence"]), 1),
                    "risk_level": record["risk_level"]
                })
            
            return {
                "history": history_records,
                "total_count": len(history_records)
            }
            
    except Exception as e:
        logger.error(f"è·å–åˆ†æå†å²å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–åˆ†æå†å²å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)












